import streamlit as st
import google.generativeai as genai
from pathlib import Path
import PyPDF2
import io
import base64
import datetime # ìºì‹œ ìœ íš¨ ê¸°ê°„ ì„¤ì •ì„ ìœ„í•´ ì¶”ê°€

# --- ì´ˆê¸° ì„¤ì • ---

# st.session_state ì´ˆê¸°í™” (ìºì‹œ ì´ë¦„ ë³€ìˆ˜ ì¶”ê°€)
if 'analysis_complete' not in st.session_state:
    st.session_state.analysis_complete = False
if 'simulation_mode' not in st.session_state:
    st.session_state.simulation_mode = False
if 'pro_cache_name' not in st.session_state:
    st.session_state.pro_cache_name = None
if 'flash_cache_name' not in st.session_state:
    st.session_state.flash_cache_name = None
if 'messages' not in st.session_state:
    st.session_state.messages = []
if 'simulation_transcript' not in st.session_state:
    st.session_state.simulation_transcript = []
if 'life_record' not in st.session_state: st.session_state.life_record = ""
if 'cover_letter' not in st.session_state: st.session_state.cover_letter = ""
if 'initial_result' not in st.session_state: st.session_state.initial_result = ""
if 'additional_questions' not in st.session_state: st.session_state.additional_questions = ""
if 'premium_report' not in st.session_state: st.session_state.premium_report = ""
if 'model_answers' not in st.session_state: st.session_state.model_answers = ""
if 'simulation_report' not in st.session_state: st.session_state.simulation_report = ""


# Secretsì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
try:
    AI_PROMPT = st.secrets["PROMPT_SECRET"]
except (FileNotFoundError, KeyError):
    st.error("í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .streamlit/secrets.toml íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# API í‚¤ ì„¤ì •
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
    # ìƒì„¸ ë¶„ì„ìš© Pro ëª¨ë¸
    # model_pro = genai.GenerativeModel('gemini-2.5-pro')
    PRO_MODEL_NAME = 'models/gemini-2.5-pro'
    # ì‹¤ì‹œê°„ ëŒ€í™”ìš© Flash ëª¨ë¸
    # model_flash = genai.GenerativeModel('gemini-2.5-flash')
    FLASH_MODEL_NAME = 'models/gemini-2.5-flash'
except (FileNotFoundError, KeyError):
    st.error("API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .streamlit/secrets.toml íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
def extract_text_from_pdf(pdf_file):
    if pdf_file is not None:
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file.read()))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() or ""
            return text
        except Exception as e:
            st.error(f"PDF íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return None
    return None

# ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜ (ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ì•ˆì •ì ì…ë‹ˆë‹¤)
def get_image_base64(path):
    try:
        with open(path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode('utf-8')
    except FileNotFoundError:
        return None

# --- ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ (UI) ---
# --- ğŸˆ ë©”ì¸ ì•± ë¡œì§: ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ì „í™˜ ---

# ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìœ¼ë©´ ì±„íŒ… UIë¥¼ í‘œì‹œ
if st.session_state.simulation_mode:
    st.title("ğŸ¤– ì‹¤ì‹œê°„ ì••ë°• ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜")

    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if user_input := st.chat_input("ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”... ('ì¢…ë£Œ' ì…ë ¥ ì‹œ ë¦¬í¬íŠ¸ ìƒì„±)"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ê¸°ë¡
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        # AI ì‘ë‹µ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ë©´ì ‘ê´€ì´ ë‹µë³€ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒ ì§ˆë¬¸ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    # Flash ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
                    flash_cache = genai.caching.CachedContent.from_name(st.session_state.flash_cache_name)
                    model_flash_from_cache = genai.GenerativeModel.from_cached_content(flash_cache)
                    
                    # í”„ë¡¬í”„íŠ¸ì—ëŠ” ì „ì²´ ëŒ€í™” ê¸°ë¡ê³¼ ë§ˆì§€ë§‰ ë‹µë³€ë§Œ ì „ë‹¬
                    simulation_prompt = f"""
                    [í˜„ì¬ ë©´ì ‘ ëŒ€í™” ê¸°ë¡]
                    {st.session_state.messages}

                    ---
                    [ì‚¬ìš©ì ëª…ë ¹ì–´]
                    ì‚¬ìš©ìê°€ ë°©ê¸ˆ '{user_input}'ì´ë¼ê³  ë‹µë³€í–ˆìŠµë‹ˆë‹¤. ì´ ë‹µë³€ì„ ë¶„ì„í•˜ê³ , ì„¤ì •ëœ ë‚œì´ë„ì™€ í”¼ë“œë°± ëª¨ë“œì— ë§ì¶° ë‹¤ìŒ ê¼¬ë¦¬ ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
                    ë§Œì•½ ì‚¬ìš©ìê°€ 'ì¢…ë£Œ'ë¥¼ ì„ ì–¸í–ˆë‹¤ë©´, ì „ì²´ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ [ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ìµœì¢… ë¦¬í¬íŠ¸]ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
                    """
                    response = model_flash_from_cache.generate_content(simulation_prompt)
                    ai_response = response.text
                    st.markdown(ai_response)
                    st.session_state.messages.append({"role": "assistant", "content": ai_response})
                except Exception as e:
                    st.error(f"ì‹œë®¬ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    if st.button("ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ ë° ë¶„ì„ ëª¨ë“œë¡œ ëŒì•„ê°€ê¸°"):
        st.session_state.simulation_mode = False
        st.rerun()
    
 # --- âœ¨ ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ ë²„íŠ¼ (ìºì‹± ë²„ì „) ---
    if st.button("ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ ë° ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"):
        with st.spinner("ë©´ì ‘ ì „ì²´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                # 1. Pro ëª¨ë¸ ìºì‹œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
                pro_cache = genai.caching.CachedContent.from_name(st.session_state.pro_cache_name)
                model_pro_from_cache = genai.GenerativeModel.from_cached_content(pro_cache)

                # 2. ìƒˆë¡œìš´ ì •ë³´(ëŒ€í™” ê¸°ë¡, ìµœì¢… ëª…ë ¹ì–´)ë§Œ ë‹´ì•„ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
                report_prompt = f"""
                [ë©´ì ‘ ì „ì²´ ëŒ€í™” ê¸°ë¡]
                {st.session_state.messages}
                ---
                [ì‚¬ìš©ì ëª…ë ¹ì–´]
                'ì¢…ë£Œ' ëª…ë ¹ì…ë‹ˆë‹¤. ìœ„ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ [ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ìµœì¢… ë¦¬í¬íŠ¸]ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
                """
                
                # 3. ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ AIì— ë¦¬í¬íŠ¸ ìƒì„± ìš”ì²­
                response = model_pro_from_cache.generate_content(report_prompt)

                # 4. ê²°ê³¼ë¬¼ì„ session_stateì— ì €ì¥í•˜ê¸° ì „ì—, ëŒ€í™” ë‚´ìš©ì„ ë³µì‚¬í•©ë‹ˆë‹¤.
                st.session_state.simulation_transcript = st.session_state.messages
                
                # 5. ê²°ê³¼ë¬¼ì„ session_stateì— ì €ì¥
                st.session_state.simulation_report = response.text
                
                # 6. ì‹œë®¬ë ˆì´ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë° ë¶„ì„ ëª¨ë“œë¡œ ì „í™˜
                st.session_state.simulation_mode = False
                st.session_state.messages = []
                
                # 7. Flash ìºì‹œëŠ” ì‹œë®¬ë ˆì´ì…˜ì´ ëë‚˜ë©´ ì‚­ì œí•˜ì—¬ ìì› ì •ë¦¬
                if st.session_state.flash_cache_name:
                    genai.caching.CachedContent.delete(name=st.session_state.flash_cache_name)
                    st.session_state.flash_cache_name = None
                
                st.rerun()

            except Exception as e:
                st.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
else:

    # 1. ì´ë¯¸ì§€ ì‚½ì… (ì¤‘ì•™ ì •ë ¬)
    image_path = "JYC_clear.png"  # ğŸ‘ˆ ì—¬ê¸°ì— ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ì´ë¦„ì„ ì •í™•í•˜ê²Œ ì…ë ¥í•˜ì„¸ìš”.
    image_base64 = get_image_base64(image_path)

    if image_base64:
        # ì´ë¯¸ì§€ íŒŒì¼ í™•ì¥ìì— ë”°ë¼ 'image/png' ë˜ëŠ” 'image/jpeg'ë¡œ ìë™ ë³€ê²½
        file_extension = Path(image_path).suffix.lower().replace('.', '')
        if file_extension in ['jpg', 'jpeg']:
            image_type = 'image/jpeg'
        else:
            image_type = 'image/png'

        st.markdown(
            f"""
            <div style="text-align: center;">
                <img src="data:{image_type};base64,{image_base64}" 
                    alt="ë¡œê³ " style="width:180px; margin-bottom: 20px;">
            </div>
            """,
            unsafe_allow_html=True
        )
    else:
        st.warning(f"'{image_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. app.pyì™€ ê°™ì€ í´ë”ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

    # 2. ì œëª© ë° ì„¤ëª…
    st.markdown("<h1 style='text-align: center;'>ë©´ì ‘ê´€ AI</h1>", unsafe_allow_html=True)
    st.markdown("<p style='text-align: center;'>Developed by JunyoungCho</p>", unsafe_allow_html=True)

    # ì´ˆê¸° ë¶„ì„ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìœ¼ë©´ íŒŒì¼ ì—…ë¡œë“œ í™”ë©´ í‘œì‹œ
    if not st.session_state.analysis_complete:
        st.markdown("<p style='text-align: center; font-size: 1.1em;'>ë‹¹ì‹ ì˜ ì„œë¥˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ë‚ ì¹´ë¡œìš´ ì˜ˆìƒ ì§ˆë¬¸ì„ ì¶”ì¶œí•˜ê³ , ì™„ë²½í•œ ë°©ì–´ ë…¼ë¦¬ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤.</p>", unsafe_allow_html=True)
        st.divider()
        st.subheader("1. ë¶„ì„ ìë£Œ ì—…ë¡œë“œ")
        st.write("ìƒê¸°ë¶€ì™€ ìì†Œì„œ PDF íŒŒì¼ì„ ê°ê° ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        
        col1, col2 = st.columns(2)
        with col1:
            life_record_file = st.file_uploader("ğŸ“„ ìƒí™œê¸°ë¡ë¶€ PDF ì—…ë¡œë“œ", type=['pdf'])
        with col2:
            cover_letter_file = st.file_uploader("âœï¸ ìê¸°ì†Œê°œì„œ PDF ì—…ë¡œë“œ", type=['pdf'])
        
        if st.button("ì´ˆê¸° ë¶„ì„ ë° ëŒ€í‘œ ì§ˆë¬¸ ì¶”ì¶œ", use_container_width=True, type="primary"):
            # 1. íŒŒì¼ì´ ëª¨ë‘ ì—…ë¡œë“œ ë˜ì—ˆëŠ”ì§€ ë¨¼ì € í™•ì¸í•©ë‹ˆë‹¤.
            if life_record_file and cover_letter_file:
                with st.spinner("PDF íŒŒì¼ì„ ë¶„ì„í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                    # 2. í…ìŠ¤íŠ¸ ì¶”ì¶œì€ ì—¬ê¸°ì„œ ë”± í•œ ë²ˆë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.
                    life_record_text = extract_text_from_pdf(life_record_file)
                    cover_letter_text = extract_text_from_pdf(cover_letter_file)

                # 3. í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
                if life_record_text and cover_letter_text:
                    with st.spinner("AIê°€ ë¶„ì„ì„ ìœ„í•´ ì„œë¥˜ë¥¼ ìºì‹± ì¤‘ì…ë‹ˆë‹¤...(ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš” 5ë¶„ ì´ë‚´ ì™„ë£Œ)"):
                        try:
                            # --- ğŸ’¡ 1. ìºì‹œ ìƒì„± (role ì¶”ê°€) ---
                            cache = genai.caching.CachedContent.create(
                                model=PRO_MODEL_NAME,
                                system_instruction=AI_PROMPT,
                                contents=[{
                                    'role': 'user', # ğŸ‘ˆ ì´ í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ëŠ” 'user'ê°€ ì œê³µí–ˆìŒì„ ëª…ì‹œ
                                    'parts': [
                                        {'text': "--- [ì‚¬ìš©ì ì œì¶œ ìë£Œ] ---"},
                                        {'text': f"[ìƒí™œê¸°ë¡ë¶€ ë‚´ìš©]:\n{life_record_text}"},
                                        {'text': f"[ìê¸°ì†Œê°œì„œ ë‚´ìš©]:\n{cover_letter_text}"}
                                    ]
                                }],
                                ttl=datetime.timedelta(hours=1)
                            )
                            st.session_state.cache_name = cache.name
                            
                            # --- ğŸ’¡ 2. ìƒì„±ëœ ìºì‹œë¥¼ ë°”ë¡œ ì‚¬ìš©í•´ ì²« ë¶„ì„ ì‹¤í–‰ ---
                            model_pro_from_cache = genai.GenerativeModel.from_cached_content(cache)
                            response = model_pro_from_cache.generate_content("ì´ì œ ì´ˆê¸° ë¶„ì„ì„ ì‹œì‘í•˜ê³  [ì´ˆê¸° ë¶„ì„ ë³´ê³ ì„œ ë° ëŒ€í‘œ ì§ˆë¬¸ 5ê°œ]ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
                            
                            st.session_state.life_record = life_record_text
                            st.session_state.cover_letter = cover_letter_text
                            st.session_state.initial_result = response.text
                            st.session_state.analysis_complete = True
                            st.rerun()

                        except Exception as e:
                            st.error(f"ìºì‹œ ìƒì„± ë˜ëŠ” AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                # (í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ extract_text_from_pdf í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ í‘œì‹œë©ë‹ˆë‹¤)
            else:
                st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.warning("ë‘ ê°œì˜ PDF íŒŒì¼ì„ ëª¨ë‘ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    # ì´ˆê¸° ë¶„ì„ì´ ì™„ë£Œë˜ë©´ ê²°ê³¼ ë° ì¶”ê°€ ê¸°ëŠ¥ ë²„íŠ¼ í‘œì‹œ
    else:
        # --- ğŸ’¡ 3. ëª¨ë“  ê¸°ëŠ¥ì—ì„œ ìºì‹œë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë¡œì§ ìˆ˜ì • ---
        st.subheader("ğŸ“Š ì´ˆê¸° ë¶„ì„ ë³´ê³ ì„œ ë° ëŒ€í‘œ ì§ˆë¬¸")
        st.markdown(st.session_state.initial_result)
        
        # ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘ (ìºì‹œ ì‚­ì œ í¬í•¨) ë²„íŠ¼
        if st.button("ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘í•˜ê¸°", type="secondary"):
            if st.session_state.cache_name:
                with st.spinner("ê¸°ì¡´ ìºì‹œë¥¼ ì‚­ì œí•˜ëŠ” ì¤‘..."):
                    genai.caching.CachedContent.delete(name=st.session_state.cache_name)
            # ëª¨ë“  ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            for key in st.session_state.keys():
                del st.session_state[key]
            st.rerun()

        st.divider()
        st.subheader("âš™ï¸ í”„ë¦¬ë¯¸ì—„ ê¸°ëŠ¥")
        st.write("ì„œë¥˜ì˜ ëª¨ë“  ì ì¬ì  ì•½ì ì„ íŒŒê³ ë“œëŠ” ì‹¬ì¸µ ë¶„ì„ì„ í†µí•´ ë©´ì ‘ì„ ì™„ë²½í•˜ê²Œ ëŒ€ë¹„í•˜ì„¸ìš”.")


        # ìºì‹œëœ ì½˜í…ì¸ ë¡œ Pro ëª¨ë¸ ì´ˆê¸°í™”
        cached_content_pro = genai.caching.get_cached_content(name=st.session_state.cache_name)
        model_pro_from_cache = genai.GenerativeModel.from_cached_content(cached_content_pro)

        col1, col2, col3 = st.columns(3)
        with col1:
            if st.button("ì¶”ê°€ ì§ˆë¬¸ ì¶”ì¶œ (20ê°œ)", use_container_width=True):
                with st.spinner("ì„œë¥˜ì˜ íŠ¹ì • ë¬¸ì¥ê³¼ ë‹¨ì–´ê¹Œì§€ íŒŒê³ ë“œëŠ” 20ê°œì˜ ì •ë°€ íƒ€ê²© ì§ˆë¬¸ì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤..."):
                    response = model_pro_from_cache.generate_content("On command: 'ì¶”ê°€ì§ˆë¬¸ì¶”ì¶œ'")
                    st.session_state.additional_questions = response.text
        with col2:
            if st.button("í”„ë¦¬ë¯¸ì—„ ì¢…í•© ì „ëµ ë³´ê³ ì„œ", use_container_width=True):
                with st.spinner("í•©ê²© ì‹œë‚˜ë¦¬ì˜¤ì™€ 4D ì „ëµ ë¶„ì„ì„ í¬í•¨í•œ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±ì¤‘ì…ë‹ˆë‹¤..."):
                    response = model_pro_from_cache.generate_content("On command: 'ìƒê¸°ë¶€ë¶„ì„'")
                    st.session_state.premium_report = response.text
        with col3:
                if st.button("ì „ëµì  ëª¨ë²” ë‹µì•ˆ ìƒì„±", use_container_width=True):
                    if not st.session_state.initial_result:
                        st.warning("ëª¨ë²” ë‹µì•ˆì„ ìƒì„±í•˜ë ¤ë©´ ë¨¼ì € ì´ˆê¸° ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    else:
                        with st.spinner("ëª¨ë“  ì§ˆë¬¸ê³¼ ë³´ê³ ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ëª¨ë²” ë‹µì•ˆì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤..."):
                            try:
                                # 1. Pro ëª¨ë¸ ìºì‹œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
                                pro_cache = genai.caching.CachedContent.from_name(st.session_state.pro_cache_name)
                                model_pro_from_cache = genai.GenerativeModel.from_cached_content(pro_cache)

                                # 2. session_stateì— ì €ì¥ëœ ëª¨ë“  ê²°ê³¼ë¬¼ì„ ì·¨í•©í•©ë‹ˆë‹¤.
                                dynamic_context = [st.session_state.initial_result]
                                if st.session_state.additional_questions:
                                    dynamic_context.append(st.session_state.additional_questions)
                                if st.session_state.premium_report:
                                    dynamic_context.append(st.session_state.premium_report)

                                # âœ¨ ì˜¤ë¥˜ í•´ê²°: .join() ê²°ê³¼ë¥¼ ë³„ë„ì˜ ë³€ìˆ˜ë¡œ ë¨¼ì € ë§Œë“­ë‹ˆë‹¤.
                                joined_context = "\n\n---\n\n".join(dynamic_context)
                                
                                # 3. ìƒˆë¡œìš´ ì •ë³´(ë™ì  ì»¨í…ìŠ¤íŠ¸, ìµœì¢… ëª…ë ¹ì–´)ë§Œ ë‹´ì•„ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
                                answer_prompt = f"""
                                [ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ë¶„ì„ ë° ì§ˆë¬¸ ëª©ë¡]
                                {joined_context}
                        
                                ---
                                [ì‚¬ìš©ì ëª…ë ¹ì–´]
                                On command: 'ëª¨ë²”ë‹µì•ˆìƒì„±'
                                ì´ì œ ìœ„ ë¶„ì„ ë‚´ìš©ê³¼ ì§ˆë¬¸ ì „ì²´ì— ëŒ€í•œ [ì „ëµì  ëª¨ë²” ë‹µì•ˆ íŒ¨í‚¤ì§€]ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
                                """

                                # 4. ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ AIì— ìš”ì²­
                                response = model_pro_from_cache.generate_content(answer_prompt)
                                st.session_state.model_answers = response.text

                            except Exception as e:
                                st.error(f"ëª¨ë²” ë‹µì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

        st.divider()

        # --- âœ¨ ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ì„¹ì…˜ (ìºì‹œ ë‚´ìš© ê°•í™” ë²„ì „) ---
        st.subheader("ğŸ¤– ì‹¤ì‹œê°„ ì••ë°• ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘")
        st.write("AI ë©´ì ‘ê´€ê³¼ í•¨ê»˜ ì‹¤ì œì™€ ê°™ì€ ì••ë°• ë©´ì ‘ì„ ê²½í—˜í•˜ê³ , ë‹¹ì‹ ì˜ ë…¼ë¦¬ë¥¼ ìµœì¢… ì ê²€í•˜ì„¸ìš”.")

        difficulty = st.slider("ë©´ì ‘ ë‚œì´ë„ ì„¤ì • (1~10)", 1, 10, 5)
        feedback_mode = st.toggle("ë‹µë³€ í›„ ì‹¤ì‹œê°„ í”¼ë“œë°± ON/OFF", value=True)
        
        if st.button("ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘í•˜ê¸°", use_container_width=True, type="primary"):
            with st.spinner("ì‹¤ì‹œê°„ ëŒ€í™”ë¥¼ ìœ„í•´ Flash ëª¨ë¸ìš© ìºì‹œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... (ëª¨ë“  ë¶„ì„ ë‚´ìš© í¬í•¨)"):
                try:
                    # --- ğŸ’¡ Flash ëª¨ë¸ìš© ìºì‹œì— ë‹´ì„ ì½˜í…ì¸  ëª©ë¡ ì¤€ë¹„ ---
                    flash_cache_contents = [
                        {'parts': [{'text': f"[ìƒí™œê¸°ë¡ë¶€ ë‚´ìš©]:\n{st.session_state.life_record}"}]},
                        {'parts': [{'text': f"[ìê¸°ì†Œê°œì„œ ë‚´ìš©]:\n{st.session_state.cover_letter}"}]},
                        {'parts': [{'text': f"[ì´ˆê¸° ë¶„ì„ ë³´ê³ ì„œ ë° ëŒ€í‘œ ì§ˆë¬¸]:\n{st.session_state.initial_result}"}]}
                    ]
                    
                    # ì¶”ê°€ ì§ˆë¬¸ì´ ìƒì„±ë˜ì—ˆë‹¤ë©´, ìºì‹œ ë‚´ìš©ì— ì¶”ê°€
                    if st.session_state.additional_questions:
                        flash_cache_contents.append(
                            {'parts': [{'text': f"[ì‹¬ì¸µ í•´ë¶€ ì§ˆë¬¸ (20ê°œ)]:\n{st.session_state.additional_questions}"}]}
                        )

                    # --- ğŸ’¡ Flash ëª¨ë¸ìš© ìºì‹œ ìƒì„± (role ì¶”ê°€) ---
                    flash_cache = genai.caching.CachedContent.create(
                        model=FLASH_MODEL_NAME,
                        system_instruction=AI_PROMPT,
                        contents=[{
                            'role': 'user', # ğŸ‘ˆ ëª¨ë“  ì‚¬ì „ ì •ë³´ë¥¼ 'user'ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
                            'parts': [item['parts'][0] for item in flash_cache_contents]
                        }],
                        ttl=datetime.timedelta(hours=1)
                    )
                    st.session_state.flash_cache_name = flash_cache.name

                    # --- ğŸ’¡ ìƒì„±ëœ Flash ìºì‹œë¡œ ì²« ì§ˆë¬¸ ìƒì„± ---
                    model_flash_from_cache = genai.GenerativeModel.from_cached_content(flash_cache)
                    feedback_status = "ON" if feedback_mode else "OFF"
                    start_prompt = f"""
                    [ì‚¬ìš©ì ëª…ë ¹ì–´]
                    On command: 'ë©´ì ‘ì‹œë®¬ë ˆì´ì…˜ì‹œì‘'
                    Parameters: difficulty: {difficulty}, feedback_mode: '{feedback_status}'
                    
                    ì´ì œ ë‹¹ì‹ ì—ê²Œ ì œê³µëœ ëª¨ë“  ì‚¬ì „ ì •ë³´(ì„œë¥˜ ì›ë³¸, ì´ˆê¸° ë¶„ì„, ì‹¬ì¸µ ì§ˆë¬¸ ëª©ë¡)ë¥¼ ì™„ë²½íˆ ìˆ™ì§€í•œ ìƒíƒœì—ì„œ, 
                    ìœ„ íŒŒë¼ë¯¸í„°ì— ë§ì¶° [ì‹œì‘ ë©˜íŠ¸]ì™€ í•¨ê»˜ ì²« ë²ˆì§¸ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
                    """
                    response = model_flash_from_cache.generate_content(start_prompt)
                    first_question = response.text
                    
                    st.session_state.messages = [{"role": "assistant", "content": first_question}]
                    st.session_state.simulation_mode = True
                    st.rerun()
                except Exception as e:
                    st.error(f"ì‹œë®¬ë ˆì´ì…˜ ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")



# --- âœ¨ ê²°ê³¼ë¬¼ ì‹œê°í™” ê°œì„  ---
        st.markdown("---") # ì‹œê°ì  êµ¬ë¶„ì„ 
        st.subheader("ğŸ“‹ ë¶„ì„ ê²°ê³¼ ë° í”„ë¦¬ë¯¸ì—„ ë¦¬í¬íŠ¸")
        st.write("ì•„ë˜ ì„¹ì…˜ì„ í´ë¦­í•˜ì—¬ ê° ë¶„ì„ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")

        # 1. ì´ˆê¸° ë¶„ì„ ë³´ê³ ì„œ ë° ëŒ€í‘œ ì§ˆë¬¸
        if st.session_state.initial_result:
            with st.expander("ğŸ“Š ì´ˆê¸° ë¶„ì„ ë³´ê³ ì„œ ë° ëŒ€í‘œ ì§ˆë¬¸", expanded=True): # ê¸°ë³¸ì ìœ¼ë¡œ ì—´ë ¤ìˆê²Œ
                st.markdown(st.session_state.initial_result)

        # 2. í”„ë¦¬ë¯¸ì—„ ì¢…í•© ì „ëµ ë³´ê³ ì„œ
        if st.session_state.premium_report:
            with st.expander("ğŸ“‘ í”„ë¦¬ë¯¸ì—„ ì¢…í•© ì „ëµ ë³´ê³ ì„œ"):
                st.markdown(st.session_state.premium_report)

        # 3. ì‹¬ì¸µ í•´ë¶€ ì§ˆë¬¸ (20ê°œ)
        if st.session_state.additional_questions:
            with st.expander("ğŸ”¬ ì‹¬ì¸µ í•´ë¶€ ì§ˆë¬¸ (20ê°œ)"):
                st.markdown(st.session_state.additional_questions)

        # 4. ì „ëµì  ëª¨ë²” ë‹µì•ˆ íŒ¨í‚¤ì§€
        if st.session_state.model_answers:
            with st.expander("ğŸ’¡ ì „ëµì  ëª¨ë²” ë‹µì•ˆ íŒ¨í‚¤ì§€"):
                st.markdown(st.session_state.model_answers)

        # 5. ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ìµœì¢… ë¦¬í¬íŠ¸ (ì‹œë®¬ë ˆì´ì…˜ í›„ ìƒì„±)
        if st.session_state.simulation_report:
            with st.expander("ğŸ“‹ ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ìµœì¢… ë¦¬í¬íŠ¸", expanded=True): # ì‹œë®¬ë ˆì´ì…˜ í›„ ê°•ì¡°
                st.markdown(st.session_state.simulation_report)
                
        # 6. ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ëŒ€í™” ê¸°ë¡ (ì‹ ê·œ ì¶”ê°€)
        if st.session_state.simulation_transcript:
            with st.expander("ğŸ’¬ ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ì „ì²´ ëŒ€í™” ë‹¤ì‹œë³´ê¸°"):
                for message in st.session_state.simulation_transcript:
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"])

        st.markdown("---") # ì‹œê°ì  êµ¬ë¶„ì„ 