import streamlit as st
import google.generativeai as genai
from pathlib import Path
import PyPDF2
import io
import base64
import datetime

# --- ì´ˆê¸° ì„¤ì • ---

# ğŸ’¡ ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ ì •ë¦¬ ë° ëª…í™•í™”
st.session_state.setdefault('analysis_complete', False)
st.session_state.setdefault('simulation_mode', False)
st.session_state.setdefault('pro_cache_name', None)
st.session_state.setdefault('flash_cache_name', None)
st.session_state.setdefault('messages', [])
st.session_state.setdefault('simulation_transcript', [])
st.session_state.setdefault('life_record', "")
st.session_state.setdefault('cover_letter', "")
st.session_state.setdefault('initial_result', "")
st.session_state.setdefault('additional_questions', "")
st.session_state.setdefault('premium_report', "")
st.session_state.setdefault('model_answers', "")
st.session_state.setdefault('simulation_report', "")

# Secretsì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
try:
    AI_PROMPT = st.secrets["PROMPT_SECRET"]
except (FileNotFoundError, KeyError):
    st.error("í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .streamlit/secrets.toml íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# API í‚¤ ë° ëª¨ë¸ ì´ë¦„ ì„¤ì •
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
    # ğŸ’¡ (ì¤‘ìš”) CachedContentë¥¼ ì§€ì›í•˜ëŠ” ë²„ì „ ëª…ì‹œ ëª¨ë¸ë¡œ ìˆ˜ì •
    PRO_MODEL_NAME = 'models/gemini-2.5-pro'
    FLASH_MODEL_NAME = 'models/gemini-2.5-flash'
except (FileNotFoundError, KeyError):
    st.error("API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .streamlit/secrets.toml íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
def extract_text_from_pdf(pdf_file):
    if pdf_file is not None:
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file.read()))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() or ""
            return text
        except Exception as e:
            st.error(f"PDF íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return None
    return None

# ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©í•˜ëŠ” í•¨ìˆ˜
def get_image_base64(path):
    try:
        with open(path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode('utf-8')
    except FileNotFoundError:
        return None

def parse_questions_from_report(text_block: str, start_marker: str) -> str:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë¸”ë¡ì—ì„œ íŠ¹ì • ì‹œì‘ ë§ˆì»¤ ì´í›„ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        text_block (str): AIê°€ ìƒì„±í•œ ì „ì²´ í…ìŠ¤íŠ¸ (e.g., st.session_state.initial_result)
        start_marker (str): ì¶”ì¶œì„ ì‹œì‘í•  ê¸°ì¤€ì´ ë˜ëŠ” ë¬¸ìì—´ (e.g., "ëŒ€í‘œ ì§ˆë¬¸")
        
    Returns:
        str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸. ë§ˆì»¤ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì¼ë¶€ë‚˜ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•  ìˆ˜ ìˆìŒ.
    """
    try:
        # start_markerë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ•ë‹ˆë‹¤.
        parts = text_block.split(start_marker)
        
        # start_markerê°€ ì¡´ì¬í•œë‹¤ë©´, parts ë¦¬ìŠ¤íŠ¸ëŠ” 2ê°œ ì´ìƒì˜ ìš”ì†Œë¥¼ ê°€ì§‘ë‹ˆë‹¤.
        if len(parts) > 1:
            # ë‘ ë²ˆì§¸ ë¶€ë¶„(parts[1])ì´ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ë‚´ìš©ì…ë‹ˆë‹¤.
            # .strip()ìœ¼ë¡œ ì•ë’¤ ê³µë°±ì´ë‚˜ ì¤„ë°”ê¿ˆì„ ì œê±°í•´ì¤ë‹ˆë‹¤.
            return parts[1].strip()
        else:
            # ë§ˆì»¤ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, ê·¸ëƒ¥ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ì—¬ ì˜¤ë¥˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
            return text_block
            
    except Exception:
        # ë§Œì•½ ì˜ˆì™¸ê°€ ë°œìƒí•˜ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return text_block
# --- ğŸˆ ë©”ì¸ ì•± ë¡œì§ ---

if st.session_state.simulation_mode:
    st.title("ğŸ¤– ì‹¤ì‹œê°„ ì••ë°• ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜")

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if user_input := st.chat_input("ë‹µë³€ì„ ì…ë ¥í•˜ì„¸ìš”... ('ì¢…ë£Œ' ì…ë ¥ ì‹œ ë¦¬í¬íŠ¸ ìƒì„±)"):
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        with st.chat_message("assistant"):
            with st.spinner("ë©´ì ‘ê´€ì´ ë‹µë³€ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒ ì§ˆë¬¸ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    # ğŸ’¡ (ì¤‘ìš”) ì˜¬ë°”ë¥¸ í•¨ìˆ˜ë¡œ Flash ìºì‹œ ë¶ˆëŸ¬ì˜¤ê¸°
                    flash_cache = genai.caching.CachedContent.get(name=st.session_state.flash_cache_name)
                    model_flash_from_cache = genai.GenerativeModel.from_cached_content(flash_cache)
                    
                    simulation_prompt = f"""
                    [í˜„ì¬ ë©´ì ‘ ëŒ€í™” ê¸°ë¡]
                    {st.session_state.messages}
                    ---
                    [ì‚¬ìš©ì ëª…ë ¹ì–´]
                    ì‚¬ìš©ìê°€ ë°©ê¸ˆ '{user_input}'ì´ë¼ê³  ë‹µë³€í–ˆìŠµë‹ˆë‹¤. ì´ ë‹µë³€ì„ ë¶„ì„í•˜ê³ , ì„¤ì •ëœ ë‚œì´ë„ì™€ í”¼ë“œë°± ëª¨ë“œì— ë§ì¶° ë‹¤ìŒ ê¼¬ë¦¬ ì§ˆë¬¸ì´ë‚˜ í”¼ë“œë°±ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
                    """
                    response = model_flash_from_cache.generate_content(simulation_prompt)
                    ai_response = response.text
                    st.markdown(ai_response)
                    st.session_state.messages.append({"role": "assistant", "content": ai_response})
                except Exception as e:
                    st.error(f"ì‹œë®¬ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    if st.button("ì‹œë®¬ë ˆì´ì…˜ ì¢…ë£Œ ë° ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"):
        with st.spinner("ë©´ì ‘ ì „ì²´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            try:
                # ğŸ’¡ (ì¤‘ìš”) ì˜¬ë°”ë¥¸ í•¨ìˆ˜ë¡œ Pro ìºì‹œ ë¶ˆëŸ¬ì˜¤ê¸°
                pro_cache = genai.caching.CachedContent.get(name=st.session_state.pro_cache_name)
                model_pro_from_cache = genai.GenerativeModel.from_cached_content(pro_cache)

                report_prompt = f"""
                [ë©´ì ‘ ì „ì²´ ëŒ€í™” ê¸°ë¡]
                {st.session_state.messages}
                ---
                [ì‚¬ìš©ì ëª…ë ¹ì–´]
                'ì¢…ë£Œ' ëª…ë ¹ì…ë‹ˆë‹¤. ìœ„ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ [ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ìµœì¢… ë¦¬í¬íŠ¸]ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
                """
                response = model_pro_from_cache.generate_content(report_prompt)
                
                st.session_state.simulation_transcript = st.session_state.messages.copy()
                st.session_state.simulation_report = response.text
                st.session_state.simulation_mode = False
                st.session_state.messages = []
                st.rerun()

            except Exception as e:
                st.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

else: # ë¶„ì„ ëª¨ë“œ UI
    # ... (ì´ë¯¸ì§€, ì œëª© ë“± UI ë¶€ë¶„ì€ ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
    image_path = "JYC_clear.png"
    image_base64 = get_image_base64(image_path)
    if image_base64:
        st.markdown(f"""<div style="text-align: center;"><img src="data:image/png;base64,{image_base64}" alt="ë¡œê³ " style="width:180px; margin-bottom: 20px;"></div>""", unsafe_allow_html=True)
    st.markdown("<h1 style='text-align: center;'>ë©´ì ‘ê´€ AI</h1>", unsafe_allow_html=True)
    st.markdown("<p style='text-align: center;'>Developed by JunyoungCho</p>", unsafe_allow_html=True)
    
    if not st.session_state.analysis_complete:
        st.markdown("<p style='text-align: center; font-size: 1.1em;'>ë‹¹ì‹ ì˜ ì„œë¥˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ë‚ ì¹´ë¡œìš´ ì˜ˆìƒ ì§ˆë¬¸ì„ ì¶”ì¶œí•˜ê³ , ì™„ë²½í•œ ë°©ì–´ ë…¼ë¦¬ë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤.</p>", unsafe_allow_html=True)
        st.divider()
        st.subheader("1. ë¶„ì„ ìë£Œ ì—…ë¡œë“œ")
        
        col1, col2 = st.columns(2)
        with col1:
            life_record_file = st.file_uploader("ğŸ“„ ìƒí™œê¸°ë¡ë¶€ PDF ì—…ë¡œë“œ", type=['pdf'])
        with col2:
            cover_letter_file = st.file_uploader("âœï¸ ìê¸°ì†Œê°œì„œ PDF ì—…ë¡œë“œ", type=['pdf'])
        
        if st.button("ì´ˆê¸° ë¶„ì„ ë° ëŒ€í‘œ ì§ˆë¬¸ ì¶”ì¶œ", use_container_width=True, type="primary"):
            if life_record_file and cover_letter_file:
                with st.spinner("PDF í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                    life_record_text = extract_text_from_pdf(life_record_file)
                    cover_letter_text = extract_text_from_pdf(cover_letter_file)

                if life_record_text and cover_letter_text:
                    st.session_state.life_record = life_record_text
                    st.session_state.cover_letter = cover_letter_text
                    
                    with st.spinner("AI ë¶„ì„ì„ ìœ„í•´ Pro/Flash ëª¨ë¸ìš© ìºì‹œë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                        try:
                            # ğŸ’¡ Proì™€ Flash ìºì‹œì— ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•  ì½˜í…ì¸  ì •ì˜
                            cache_contents = [{
                                'role': 'user',
                                'parts': [
                                    {'text': "--- [ì‚¬ìš©ì ì œì¶œ ìë£Œ] ---"},
                                    {'text': f"[ìƒí™œê¸°ë¡ë¶€ ë‚´ìš©]:\n{life_record_text}"},
                                    {'text': f"[ìê¸°ì†Œê°œì„œ ë‚´ìš©]:\n{cover_letter_text}"}
                                ]
                            }]

                            # ğŸ’¡ Pro ëª¨ë¸ìš© ìºì‹œ ìƒì„±
                            pro_cache = genai.caching.CachedContent.create(
                                model=PRO_MODEL_NAME,
                                system_instruction=AI_PROMPT,
                                contents=cache_contents,
                                ttl=datetime.timedelta(hours=1)
                            )
                            st.session_state.pro_cache_name = pro_cache.name
                            st.info("Pro ëª¨ë¸ìš© ìºì‹œ ìƒì„± ì™„ë£Œ!")

                            # ğŸ’¡ Flash ëª¨ë¸ìš© ìºì‹œë„ ë™ì‹œì— ìƒì„±
                            flash_cache = genai.caching.CachedContent.create(
                                model=FLASH_MODEL_NAME,
                                system_instruction=AI_PROMPT,
                                contents=cache_contents,
                                ttl=datetime.timedelta(hours=1)
                            )
                            st.session_state.flash_cache_name = flash_cache.name
                            st.info("Flash ëª¨ë¸ìš© ìºì‹œ ìƒì„± ì™„ë£Œ!")
                            
                            # ğŸ’¡ ìƒì„±ëœ Pro ìºì‹œë¥¼ ë°”ë¡œ ì‚¬ìš©í•´ ì²« ë¶„ì„ ì‹¤í–‰
                            model_pro_from_cache = genai.GenerativeModel.from_cached_content(pro_cache)
                            response = model_pro_from_cache.generate_content("ì´ì œ ì´ˆê¸° ë¶„ì„ì„ ì‹œì‘í•˜ê³  [ì´ˆê¸° ë¶„ì„ ë³´ê³ ì„œ ë° ëŒ€í‘œ ì§ˆë¬¸ 5ê°œ]ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
                            
                            st.session_state.initial_result = response.text
                            st.session_state.analysis_complete = True
                            st.rerun()

                        except Exception as e:
                            st.error(f"ìºì‹œ ìƒì„± ë˜ëŠ” AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                else:
                    st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.warning("ë‘ ê°œì˜ PDF íŒŒì¼ì„ ëª¨ë‘ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else: # ë¶„ì„ ì™„ë£Œ í›„ UI
        st.subheader("ğŸ“Š ì´ˆê¸° ë¶„ì„ ë³´ê³ ì„œ ë° ëŒ€í‘œ ì§ˆë¬¸")
        st.markdown(st.session_state.initial_result)
        
        if st.button("ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘í•˜ê¸°", type="secondary"):
            with st.spinner("ê¸°ì¡´ ìºì‹œë¥¼ ì‚­ì œí•˜ëŠ” ì¤‘..."):
                try:
                    # ğŸ’¡ Pro/Flash ìºì‹œ ëª¨ë‘ ì‚­ì œ
                    if st.session_state.pro_cache_name:
                        genai.caching.CachedContent.delete(name=st.session_state.pro_cache_name)
                    if st.session_state.flash_cache_name:
                        genai.caching.CachedContent.delete(name=st.session_state.flash_cache_name)
                except Exception as e:
                    st.warning(f"ìºì‹œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.rerun()

        st.divider()
        st.subheader("âš™ï¸ í”„ë¦¬ë¯¸ì—„ ê¸°ëŠ¥")
        
        try:
            # ğŸ’¡ (ì¤‘ìš”) ì˜¬ë°”ë¥¸ í•¨ìˆ˜ì™€ ë³€ìˆ˜ ì´ë¦„ìœ¼ë¡œ Pro ìºì‹œ ë¶ˆëŸ¬ì˜¤ê¸°
            pro_cache = genai.caching.CachedContent.get(name=st.session_state.pro_cache_name)
            model_pro_from_cache = genai.GenerativeModel.from_cached_content(pro_cache)

            col1, col2, col3 = st.columns(3)
            with col1:
                if st.button("ì¶”ê°€ ì§ˆë¬¸ ì¶”ì¶œ (20ê°œ)", use_container_width=True):
                    with st.spinner("ì„œë¥˜ì˜ íŠ¹ì • ë¬¸ì¥ê³¼ ë‹¨ì–´ê¹Œì§€ íŒŒê³ ë“œëŠ” 20ê°œì˜ ì •ë°€ íƒ€ê²© ì§ˆë¬¸ì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤......"):
                        response = model_pro_from_cache.generate_content("On command: 'ì¶”ê°€ì§ˆë¬¸ì¶”ì¶œ'")
                        st.session_state.additional_questions = response.text
                        st.rerun()
            with col2:
                if st.button("í”„ë¦¬ë¯¸ì—„ ì¢…í•© ì „ëµ ë³´ê³ ì„œ", use_container_width=True):
                    with st.spinner("í•©ê²© ì‹œë‚˜ë¦¬ì˜¤ì™€ 4D ì „ëµ ë¶„ì„ì„ í¬í•¨í•œ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±ì¤‘ì…ë‹ˆë‹¤..."):
                        response = model_pro_from_cache.generate_content("On command: 'ìƒê¸°ë¶€ë¶„ì„'")
                        st.session_state.premium_report = response.text
                        st.rerun()
            with col3:
                if st.button("ì „ëµì  ëª¨ë²” ë‹µì•ˆ ìƒì„±", use_container_width=True):
                    with st.spinner("ëª¨ë“  ì§ˆë¬¸ê³¼ ë³´ê³ ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ëª¨ë²” ë‹µì•ˆì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤..."):
                        # ... (ëª¨ë²” ë‹µì•ˆ ìƒì„± ë¡œì§)
                        all_questions = st.session_state.initial_result + "\n\n" + st.session_state.additional_questions
                        # ğŸ’¡ 1. ì´ˆê¸° ê²°ê³¼ì—ì„œ 'ëŒ€í‘œ ì§ˆë¬¸' ë¶€ë¶„ë§Œ íŒŒì‹±í•©ë‹ˆë‹¤.
                        # "ëŒ€í‘œ ì§ˆë¬¸" ì´ë¼ëŠ” í‚¤ì›Œë“œê°€ AI ìƒì„± ê²°ê³¼ì— ë”°ë¼ ì¡°ê¸ˆì”© ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.
                        # (ì˜ˆ: "ëŒ€í‘œ ì§ˆë¬¸ 5ê°œ", "ëŒ€í‘œ ì˜ˆìƒ ì§ˆë¬¸" ë“±)
                        initial_questions = parse_questions_from_report(
                            st.session_state.initial_result, 
                            start_marker="ëŒ€í‘œ ì˜ˆìƒ ì§ˆë¬¸" 
                        )

                        # ğŸ’¡ 2. íŒŒì‹±ëœ ì§ˆë¬¸ê³¼ ì¶”ê°€ ì§ˆë¬¸ì„ í•©ì¹©ë‹ˆë‹¤.
                        all_questions = [initial_questions] # íŒŒì‹±ëœ ê²°ê³¼
                        if st.session_state.additional_questions:
                            all_questions.append(st.session_state.additional_questions)
                        
                        questions_context = "\n\n---\n\n".join(all_questions)

                        # ğŸ’¡ 3. í›¨ì”¬ ê°€ë²¼ì›Œì§„ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
                        answer_prompt = f"""
                        [ë‹µë³€í•´ì•¼ í•  ì§ˆë¬¸ ëª©ë¡]
                        {questions_context}

                        ---
                        [ì‚¬ìš©ì ëª…ë ¹ì–´]
                        On command: 'ëª¨ë²”ë‹µì•ˆìƒì„±'
                        # ... (ì´í•˜ ë™ì¼)
                        """

                        response = model_pro_from_cache.generate_content(answer_prompt)
                        st.session_state.model_answers = response.text
                        st.rerun()

        except Exception as e:
            st.error(f"í”„ë¦¬ë¯¸ì—„ ê¸°ëŠ¥ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            st.warning("ìºì‹œê°€ ë§Œë£Œë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘í•˜ê¸°'ë¥¼ ëˆŒëŸ¬ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")


        st.divider()

        st.subheader("ğŸ¤– ì‹¤ì‹œê°„ ì••ë°• ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘")
        difficulty = st.slider("ë©´ì ‘ ë‚œì´ë„ ì„¤ì • (1~10)", 1, 10, 5)
        feedback_mode = st.toggle("ë‹µë³€ í›„ ì‹¤ì‹œê°„ í”¼ë“œë°± ON/OFF", value=True)
        
        if st.button("ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘í•˜ê¸°", use_container_width=True, type="primary"):
            try:
                # ğŸ’¡ ìºì‹œëŠ” ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë°”ë¡œ Flash ìºì‹œë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©
                flash_cache = genai.caching.CachedContent.get(name=st.session_state.flash_cache_name)
                model_flash_from_cache = genai.GenerativeModel.from_cached_content(flash_cache)
                
                feedback_status = "ON" if feedback_mode else "OFF"
                start_prompt = f"""
                [ì‚¬ìš©ì ëª…ë ¹ì–´]
                On command: 'ë©´ì ‘ì‹œë®¬ë ˆì´ì…˜ì‹œì‘'
                Parameters: difficulty: {difficulty}, feedback_mode: '{feedback_status}'
                ì´ì œ ë‹¹ì‹ ì—ê²Œ ì œê³µëœ ì„œë¥˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì²« ë²ˆì§¸ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
                """
                with st.spinner("AI ë©´ì ‘ê´€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
                    response = model_flash_from_cache.generate_content(start_prompt)
                    first_question = response.text
                
                st.session_state.messages = [{"role": "assistant", "content": first_question}]
                st.session_state.simulation_mode = True
                st.rerun()
            except Exception as e:
                st.error(f"ì‹œë®¬ë ˆì´ì…˜ ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                st.warning("ìºì‹œê°€ ë§Œë£Œë˜ì—ˆê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 'ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘í•˜ê¸°'ë¥¼ ëˆŒëŸ¬ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        
        # --- ê²°ê³¼ë¬¼ ì‹œê°í™” ë¶€ë¶„ (ë³€ê²½ ì—†ìŒ) ---
        if st.session_state.initial_result or st.session_state.premium_report or st.session_state.additional_questions or st.session_state.model_answers or st.session_state.simulation_report:
            st.markdown("---")
            st.subheader("ğŸ“‹ ë¶„ì„ ê²°ê³¼ ë° ë¦¬í¬íŠ¸")

            if st.session_state.initial_result:
                with st.expander("ğŸ“Š ì´ˆê¸° ë¶„ì„ ë³´ê³ ì„œ ë° ëŒ€í‘œ ì§ˆë¬¸", expanded=True):
                    st.markdown(st.session_state.initial_result)
            if st.session_state.premium_report:
                with st.expander("ğŸ“‘ í”„ë¦¬ë¯¸ì—„ ì¢…í•© ì „ëµ ë³´ê³ ì„œ"):
                    st.markdown(st.session_state.premium_report)
            if st.session_state.additional_questions:
                with st.expander("ğŸ”¬ ì‹¬ì¸µ í•´ë¶€ ì§ˆë¬¸ (20ê°œ)"):
                    st.markdown(st.session_state.additional_questions)
            if st.session_state.model_answers:
                with st.expander("ğŸ’¡ ì „ëµì  ëª¨ë²” ë‹µì•ˆ íŒ¨í‚¤ì§€"):
                    st.markdown(st.session_state.model_answers)
            if st.session_state.simulation_report:
                with st.expander("ğŸ“‹ ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ìµœì¢… ë¦¬í¬íŠ¸", expanded=True):
                    st.markdown(st.session_state.simulation_report)
            if st.session_state.simulation_transcript:
                with st.expander("ğŸ’¬ ë©´ì ‘ ì‹œë®¬ë ˆì´ì…˜ ì „ì²´ ëŒ€í™” ë‹¤ì‹œë³´ê¸°"):
                    for message in st.session_state.simulation_transcript:
                        with st.chat_message(message["role"]):
                            st.markdown(message["content"])